{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/popelucha/digital-humanities/blob/main/DH_3_Text_Analysis_Using_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnSW2GizjkxD"
      },
      "source": [
        "# Text Analysis using APIs\n",
        "The aim of the notebook is to begin with quantitative analysis of text data. We select a Czech text, split it into tokens, perform frequency analysis, and observe the nature of the data.\n",
        "\n",
        "Particularly, we use the Czech tagger `desamb` via Language Services API at Natural Language Processing Centre, Faculty of Informatics, Masaryk University Brno.\n",
        "\n",
        "The tagset is described in https://nlp.fi.muni.cz/raslan/2011/paper05.pdf.\n",
        "\n",
        "\n",
        "JAKUBÍČEK, Miloš, Vojtěch KOVÁŘ a Pavel ŠMERK. Czech Morphological Tagset Revisited. In Horák, Rychlý. Proceedings of Recent Advances in Slavonic Natural Language Processing 2011. Brno: Tribun EU, 2011. s. 29-42, 14 s. ISBN 978-80-263-0077-9. https://www.muni.cz/en/research/publications/959110"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4hdcv-ujkxO"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtLU9JX0jkxS"
      },
      "source": [
        "text = None\n",
        "with open('maj.txt') as f:  # modify the path if needed\n",
        "    text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdIXepWNjkxS"
      },
      "source": [
        "import requests\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_26uvJo8jkxT"
      },
      "source": [
        "# Make the API call\n",
        "In this step, we rely on the service provided by the 3rd party. It impacts not only the actual need of annotation but also reproducibility of the results.\n",
        "\n",
        "The result of the API call is a HTTP code (since the API is called via HTTP POST request). You can find the meaning of the numbers here: https://www.restapitutorial.com/httpstatuscodes.html. Usually, only few codes appear as the API call status code. For example, 200 means OK, the 404 is well known Not found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv-ncstsjkxU"
      },
      "source": [
        "data = {\"call\": \"tagger\",\n",
        "        \"lang\": \"cs\",\n",
        "        \"output\": \"json\",\n",
        "        \"text\": text.replace(';', ',')\n",
        "       }\n",
        "uri = \"https://nlp.fi.muni.cz/languageservices/service.py\"\n",
        "r = requests.post(uri, data=data)\n",
        "r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-mh5kDmjkxU"
      },
      "source": [
        "print(r.content[:1000].decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea5b0piFjkxV"
      },
      "source": [
        "Note that we convert automatically the data into JSON. JSON is one of the communication standards that is used when two machines communicate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViTgcww2jkxV"
      },
      "source": [
        "data = r.json()\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks8ibCH3jkxV"
      },
      "source": [
        "**TASK 1**: Observe the data. Without the tagger documentation, is it possible to understand the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynDT8eMVjkxW"
      },
      "source": [
        "Explanations: The tagger splits text into sentences (delimited by `<s>` and `</s>`), then into tokens. The tokens are usually separated by spaces. In case they are not (typically punctuation), the tagger adds the \"glue\" tag `<g/>`. Each token is represented by a list with three elements: word, lemma, tag.\n",
        "\n",
        "**TASK 2** Check the paper describing the tagset and try to \"decipher\" annotation for one token into human readable annotation such as \"feminine noun in plural instrumental\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKLV_xKljkxW"
      },
      "source": [
        "tokens = [token for token in data['vertical'] if len(token)==3]\n",
        "df = pd.DataFrame.from_dict({\"word\": [word for word, lemma, tag in tokens],\n",
        "                              \"lemma\": [lemma for word, lemma, tag in tokens],\n",
        "                              \"tag\": [tag for word, lemma, tag in tokens]\n",
        "                               })\n",
        "pd.options.display.max_rows = len(df)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C8ZSoxJjkxX"
      },
      "source": [
        "pos = [tag[0:2] for tag in df[\"tag\"]]\n",
        "df[\"pos\"] = pos\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcvvL1ArjkxX"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}
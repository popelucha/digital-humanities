{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/popelucha/digital-humanities/blob/main/Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFyaXi2Yilw"
      },
      "source": [
        "# Word Embeddings\n",
        "Podívejme se, jak je význam zakódovaný ve word embeddings.\n",
        "\n",
        "Dokumentace je zde: https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhF2jmlVYilw"
      },
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Dckm18ErUE"
      },
      "source": [
        "model = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P50-FEcHREIE",
        "outputId": "2492952d-5e3e-4204-b926-9f0c6661f8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'corpora': {'semeval-2016-2017-task3-subtaskBC': {'num_records': -1,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 6344358,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py',\n",
              "   'license': 'All files released for the task are free for general research use',\n",
              "   'fields': {'2016-train': ['...'],\n",
              "    '2016-dev': ['...'],\n",
              "    '2017-test': ['...'],\n",
              "    '2016-test': ['...']},\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask B and C datasets contain train+development (317 original questions, 3,169 related questions, and 31,690 comments), and test datasets in English. The description of the tasks and the collected data is given in sections 3 and 4.1 of the task paper http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf linked in section “Papers” of https://github.com/RaRe-Technologies/gensim-data/issues/18.',\n",
              "   'checksum': '701ea67acd82e75f95e1d8e62fb0ad29',\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskBC.gz',\n",
              "   'read_more': ['http://alt.qcri.org/semeval2017/task3/',\n",
              "    'http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english'],\n",
              "   'parts': 1},\n",
              "  'semeval-2016-2017-task3-subtaskA-unannotated': {'num_records': 189941,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 234373151,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py',\n",
              "   'license': 'These datasets are free for general research use.',\n",
              "   'fields': {'THREAD_SEQUENCE': '',\n",
              "    'RelQuestion': {'RELQ_CATEGORY': 'question category, according to the Qatar Living taxonomy',\n",
              "     'RELQ_DATE': 'date of posting',\n",
              "     'RELQ_ID': 'question indentifier',\n",
              "     'RELQ_USERID': 'identifier of the user asking the question',\n",
              "     'RELQ_USERNAME': 'name of the user asking the question',\n",
              "     'RelQBody': 'body of question',\n",
              "     'RelQSubject': 'subject of question'},\n",
              "    'RelComments': [{'RelCText': 'text of answer',\n",
              "      'RELC_USERID': 'identifier of the user posting the comment',\n",
              "      'RELC_ID': 'comment identifier',\n",
              "      'RELC_USERNAME': 'name of the user posting the comment',\n",
              "      'RELC_DATE': 'date of posting'}]},\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask A unannotated dataset contains 189,941 questions and 1,894,456 comments in English collected from the Community Question Answering (CQA) web forum of Qatar Living. These can be used as a corpus for language modelling.',\n",
              "   'checksum': '2de0e2f2c4f91c66ae4fcf58d50ba816',\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskA-unannotated.gz',\n",
              "   'read_more': ['http://alt.qcri.org/semeval2016/task3/',\n",
              "    'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english'],\n",
              "   'parts': 1},\n",
              "  'patent-2017': {'num_records': 353197,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 3087262469,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py',\n",
              "   'license': 'not found',\n",
              "   'description': \"Patent Grant Full Text. Contains the full text including tables, sequence data and 'in-line' mathematical expressions of each patent grant issued in 2017.\",\n",
              "   'checksum-0': '818501f0b9af62d3b88294d86d509f8f',\n",
              "   'checksum-1': '66c05635c1d3c7a19b4a335829d09ffa',\n",
              "   'file_name': 'patent-2017.gz',\n",
              "   'read_more': ['http://patents.reedtech.com/pgrbft.php'],\n",
              "   'parts': 2},\n",
              "  'quora-duplicate-questions': {'num_records': 404290,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 21684784,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py',\n",
              "   'license': 'probably https://www.quora.com/about/tos',\n",
              "   'fields': {'question1': 'the full text of each question',\n",
              "    'question2': 'the full text of each question',\n",
              "    'qid1': 'unique ids of each question',\n",
              "    'qid2': 'unique ids of each question',\n",
              "    'id': 'the id of a training set question pair',\n",
              "    'is_duplicate': 'the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise'},\n",
              "   'description': 'Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.',\n",
              "   'checksum': 'd7cfa7fbc6e2ec71ab74c495586c6365',\n",
              "   'file_name': 'quora-duplicate-questions.gz',\n",
              "   'read_more': ['https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs'],\n",
              "   'parts': 1},\n",
              "  'wiki-english-20171001': {'num_records': 4924894,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 6516051717,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py',\n",
              "   'license': 'https://dumps.wikimedia.org/legal.html',\n",
              "   'fields': {'section_texts': 'list of body of sections',\n",
              "    'section_titles': 'list of titles of sections',\n",
              "    'title': 'Title of wiki article'},\n",
              "   'description': 'Extracted Wikipedia dump from October 2017. Produced by `python -m gensim.scripts.segment_wiki -f enwiki-20171001-pages-articles.xml.bz2 -o wiki-en.gz`',\n",
              "   'checksum-0': 'a7d7d7fd41ea7e2d7fa32ec1bb640d71',\n",
              "   'checksum-1': 'b2683e3356ffbca3b6c2dca6e9801f9f',\n",
              "   'checksum-2': 'c5cde2a9ae77b3c4ebce804f6df542c2',\n",
              "   'checksum-3': '00b71144ed5e3aeeb885de84f7452b81',\n",
              "   'file_name': 'wiki-english-20171001.gz',\n",
              "   'read_more': ['https://dumps.wikimedia.org/enwiki/20171001/'],\n",
              "   'parts': 4},\n",
              "  'text8': {'num_records': 1701,\n",
              "   'record_format': 'list of str (tokens)',\n",
              "   'file_size': 33182058,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
              "   'license': 'not found',\n",
              "   'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
              "   'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
              "   'file_name': 'text8.gz',\n",
              "   'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
              "   'parts': 1},\n",
              "  'fake-news': {'num_records': 12999,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 20102776,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py',\n",
              "   'license': 'https://creativecommons.org/publicdomain/zero/1.0/',\n",
              "   'fields': {'crawled': 'date the story was archived',\n",
              "    'ord_in_thread': '',\n",
              "    'published': 'date published',\n",
              "    'participants_count': 'number of participants',\n",
              "    'shares': 'number of Facebook shares',\n",
              "    'replies_count': 'number of replies',\n",
              "    'main_img_url': 'image from story',\n",
              "    'spam_score': 'data from webhose.io',\n",
              "    'uuid': 'unique identifier',\n",
              "    'language': 'data from webhose.io',\n",
              "    'title': 'title of story',\n",
              "    'country': 'data from webhose.io',\n",
              "    'domain_rank': 'data from webhose.io',\n",
              "    'author': 'author of story',\n",
              "    'comments': 'number of Facebook comments',\n",
              "    'site_url': 'site URL from BS detector',\n",
              "    'text': 'text of story',\n",
              "    'thread_title': '',\n",
              "    'type': 'type of website (label from BS detector)',\n",
              "    'likes': 'number of Facebook likes'},\n",
              "   'description': \"News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.\",\n",
              "   'checksum': '5e64e942df13219465927f92dcefd5fe',\n",
              "   'file_name': 'fake-news.gz',\n",
              "   'read_more': ['https://www.kaggle.com/mrisdal/fake-news'],\n",
              "   'parts': 1},\n",
              "  '20-newsgroups': {'num_records': 18846,\n",
              "   'record_format': 'dict',\n",
              "   'file_size': 14483581,\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py',\n",
              "   'license': 'not found',\n",
              "   'fields': {'topic': 'name of topic (20 variant of possible values)',\n",
              "    'set': \"marker of original split (possible values 'train' and 'test')\",\n",
              "    'data': '',\n",
              "    'id': 'original id inferred from folder name'},\n",
              "   'description': 'The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.',\n",
              "   'checksum': 'c92fd4f6640a86d5ba89eaad818a9891',\n",
              "   'file_name': '20-newsgroups.gz',\n",
              "   'read_more': ['http://qwone.com/~jason/20Newsgroups/'],\n",
              "   'parts': 1},\n",
              "  '__testing_matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'checksum': '1767ac93a089b43899d54944b07d9dc5',\n",
              "   'file_name': '__testing_matrix-synopsis.gz',\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis'],\n",
              "   'parts': 1},\n",
              "  '__testing_multipart-matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'checksum-0': 'c8b0c7d8cf562b1b632c262a173ac338',\n",
              "   'checksum-1': '5ff7fc6818e9a5d9bc1cf12c35ed8b96',\n",
              "   'checksum-2': '966db9d274d125beaac7987202076cba',\n",
              "   'file_name': '__testing_multipart-matrix-synopsis.gz',\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis'],\n",
              "   'parts': 3}},\n",
              " 'models': {'fasttext-wiki-news-subwords-300': {'num_records': 999999,\n",
              "   'file_size': 1005007116,\n",
              "   'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py',\n",
              "   'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
              "   'parameters': {'dimension': 300},\n",
              "   'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).',\n",
              "   'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
              "    'https://arxiv.org/abs/1712.09405',\n",
              "    'https://arxiv.org/abs/1607.01759'],\n",
              "   'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
              "   'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
              "   'parts': 1},\n",
              "  'conceptnet-numberbatch-17-06-300': {'num_records': 1917247,\n",
              "   'file_size': 1225497562,\n",
              "   'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py',\n",
              "   'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
              "   'parameters': {'dimension': 300},\n",
              "   'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.',\n",
              "   'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
              "    'https://github.com/commonsense/conceptnet-numberbatch',\n",
              "    'http://conceptnet.io/'],\n",
              "   'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
              "   'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
              "   'parts': 1},\n",
              "  'word2vec-ruscorpora-300': {'num_records': 184973,\n",
              "   'file_size': 208427381,\n",
              "   'base_dataset': 'Russian National Corpus (about 250M words)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py',\n",
              "   'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
              "   'parameters': {'dimension': 300, 'window_size': 10},\n",
              "   'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.',\n",
              "   'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS',\n",
              "   'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
              "    'http://rusvectores.org/en/',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
              "   'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
              "   'file_name': 'word2vec-ruscorpora-300.gz',\n",
              "   'parts': 1},\n",
              "  'word2vec-google-news-300': {'num_records': 3000000,\n",
              "   'file_size': 1743563840,\n",
              "   'base_dataset': 'Google News (about 100 billion words)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py',\n",
              "   'license': 'not found',\n",
              "   'parameters': {'dimension': 300},\n",
              "   'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
              "   'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
              "    'https://arxiv.org/abs/1301.3781',\n",
              "    'https://arxiv.org/abs/1310.4546',\n",
              "    'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
              "   'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
              "   'file_name': 'word2vec-google-news-300.gz',\n",
              "   'parts': 1},\n",
              "  'glove-wiki-gigaword-50': {'num_records': 400000,\n",
              "   'file_size': 69182535,\n",
              "   'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 50},\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
              "   'file_name': 'glove-wiki-gigaword-50.gz',\n",
              "   'parts': 1},\n",
              "  'glove-wiki-gigaword-100': {'num_records': 400000,\n",
              "   'file_size': 134300434,\n",
              "   'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 100},\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
              "   'file_name': 'glove-wiki-gigaword-100.gz',\n",
              "   'parts': 1},\n",
              "  'glove-wiki-gigaword-200': {'num_records': 400000,\n",
              "   'file_size': 264336934,\n",
              "   'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 200},\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
              "   'file_name': 'glove-wiki-gigaword-200.gz',\n",
              "   'parts': 1},\n",
              "  'glove-wiki-gigaword-300': {'num_records': 400000,\n",
              "   'file_size': 394362229,\n",
              "   'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 300},\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
              "   'file_name': 'glove-wiki-gigaword-300.gz',\n",
              "   'parts': 1},\n",
              "  'glove-twitter-25': {'num_records': 1193514,\n",
              "   'file_size': 109885004,\n",
              "   'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 25},\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
              "   'file_name': 'glove-twitter-25.gz',\n",
              "   'parts': 1},\n",
              "  'glove-twitter-50': {'num_records': 1193514,\n",
              "   'file_size': 209216938,\n",
              "   'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 50},\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
              "   'file_name': 'glove-twitter-50.gz',\n",
              "   'parts': 1},\n",
              "  'glove-twitter-100': {'num_records': 1193514,\n",
              "   'file_size': 405932991,\n",
              "   'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 100},\n",
              "   'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
              "   'file_name': 'glove-twitter-100.gz',\n",
              "   'parts': 1},\n",
              "  'glove-twitter-200': {'num_records': 1193514,\n",
              "   'file_size': 795373100,\n",
              "   'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py',\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'parameters': {'dimension': 200},\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
              "   'file_name': 'glove-twitter-200.gz',\n",
              "   'parts': 1},\n",
              "  '__testing_word2vec-matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.',\n",
              "   'parameters': {'dimensions': 50},\n",
              "   'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.',\n",
              "   'read_more': [],\n",
              "   'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
              "   'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
              "   'parts': 1}}}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpuLKEwmCeet",
        "outputId": "1540d51a-f7fa-48a1-8e77-90caae4cd462"
      },
      "source": [
        "# vector for known words\n",
        "dog = model['car']\n",
        "print(dog.shape)\n",
        "print(dog)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50,)\n",
            "[ 0.47685  -0.084552  1.4641    0.047017  0.14686   0.5082   -1.2228\n",
            " -0.22607   0.19306  -0.29756   0.20599  -0.71284  -1.6288    0.17096\n",
            "  0.74797  -0.061943 -0.65766   1.3786   -0.68043  -1.7551    0.58319\n",
            "  0.25157  -1.2114    0.81343   0.094825 -1.6819   -0.64498   0.6322\n",
            "  1.1211    0.16112   2.5379    0.24852  -0.26816   0.32818   1.2916\n",
            "  0.23548   0.61465  -0.1344   -0.13237   0.27398  -0.11821   0.1354\n",
            "  0.074306 -0.61951   0.45472  -0.30318  -0.21883  -0.56054   1.1177\n",
            " -0.36595 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgi9RBol5tGr",
        "outputId": "b8a9ec08-64fc-4138-d8a6-9c789ee7b978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on KeyedVectors in module gensim.models.keyedvectors object:\n",
            "\n",
            "class KeyedVectors(gensim.utils.SaveLoad)\n",
            " |  KeyedVectors(vector_size, count=0, dtype=<class 'numpy.float32'>, mapfile_path=None)\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      KeyedVectors\n",
            " |      gensim.utils.SaveLoad\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __contains__(self, key)\n",
            " |  \n",
            " |  __getitem__(self, key_or_keys)\n",
            " |      Get vector representation of `key_or_keys`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key_or_keys : {str, list of str, int, list of int}\n",
            " |          Requested key or list-of-keys.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      numpy.ndarray\n",
            " |          Vector representation for `key_or_keys` (1D if `key_or_keys` is single key, otherwise - 2D).\n",
            " |  \n",
            " |  __init__(self, vector_size, count=0, dtype=<class 'numpy.float32'>, mapfile_path=None)\n",
            " |      Mapping between keys (such as words) and vectors for :class:`~gensim.models.Word2Vec`\n",
            " |      and related models.\n",
            " |      \n",
            " |      Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n",
            " |      \n",
            " |      To support the needs of specific models and other downstream uses, you can also set\n",
            " |      additional attributes via the :meth:`~gensim.models.keyedvectors.KeyedVectors.set_vecattr`\n",
            " |      and :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vecattr` methods.\n",
            " |      Note that all such attributes under the same `attr` name must have compatible `numpy`\n",
            " |      types, as the type and storage array for such attributes is established by the 1st time such\n",
            " |      `attr` is set.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      vector_size : int\n",
            " |          Intended number of dimensions for all contained vectors.\n",
            " |      count : int, optional\n",
            " |          If provided, vectors wil be pre-allocated for at least this many vectors. (Otherwise\n",
            " |          they can be added later.)\n",
            " |      dtype : type, optional\n",
            " |          Vector dimensions will default to `np.float32` (AKA `REAL` in some Gensim code) unless\n",
            " |          another type is provided here.\n",
            " |      mapfile_path : string, optional\n",
            " |          Currently unused.\n",
            " |  \n",
            " |  __len__(self)\n",
            " |  \n",
            " |  __setitem__(self, keys, weights)\n",
            " |      Add keys and theirs vectors in a manual way.\n",
            " |      If some key is already in the vocabulary, old vector is replaced with the new one.\n",
            " |      \n",
            " |      This method is an alias for :meth:`~gensim.models.keyedvectors.KeyedVectors.add_vectors`\n",
            " |      with `replace=True`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      keys : {str, int, list of (str or int)}\n",
            " |          keys specified by their string or int ids.\n",
            " |      weights: list of numpy.ndarray or numpy.ndarray\n",
            " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
            " |  \n",
            " |  __str__(self)\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  add_vector(self, key, vector)\n",
            " |      Add one new vector at the given key, into existing slot if available.\n",
            " |      \n",
            " |      Warning: using this repeatedly is inefficient, requiring a full reallocation & copy,\n",
            " |      if this instance hasn't been preallocated to be ready for such incremental additions.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      key: str\n",
            " |          Key identifier of the added vector.\n",
            " |      vector: numpy.ndarray\n",
            " |          1D numpy array with the vector values.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      int\n",
            " |          Index of the newly added vector, so that ``self.vectors[result] == vector`` and\n",
            " |          ``self.index_to_key[result] == key``.\n",
            " |  \n",
            " |  add_vectors(self, keys, weights, extras=None, replace=False)\n",
            " |      Append keys and their vectors in a manual way.\n",
            " |      If some key is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      keys : list of (str or int)\n",
            " |          Keys specified by string or int ids.\n",
            " |      weights: list of numpy.ndarray or numpy.ndarray\n",
            " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
            " |      replace: bool, optional\n",
            " |          Flag indicating whether to replace vectors for keys which already exist in the map;\n",
            " |          if True - replace vectors, otherwise - keep old vectors.\n",
            " |  \n",
            " |  allocate_vecattrs(self, attrs=None, types=None)\n",
            " |      Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\n",
            " |      \n",
            " |      The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\n",
            " |      even if other properties (vectors array) hasn't yet been allocated or expanded.\n",
            " |      So this allocation targets that size.\n",
            " |  \n",
            " |  closer_than(self, key1, key2)\n",
            " |      Get all keys that are closer to `key1` than `key2` is to `key1`.\n",
            " |  \n",
            " |  distance(self, w1, w2)\n",
            " |      Compute cosine distance between two keys.\n",
            " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.KeyedVectors.similarity`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      w1 : str\n",
            " |          Input key.\n",
            " |      w2 : str\n",
            " |          Input key.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Distance between `w1` and `w2`.\n",
            " |  \n",
            " |  distances(self, word_or_vector, other_words=())\n",
            " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
            " |      If `other_words` is empty, return distance between `word_or_vector` and all words in vocab.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      word_or_vector : {str, numpy.ndarray}\n",
            " |          Word or vector from which distances are to be computed.\n",
            " |      other_words : iterable of str\n",
            " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
            " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      numpy.array\n",
            " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
            " |      \n",
            " |      Raises\n",
            " |      -----\n",
            " |      KeyError\n",
            " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
            " |  \n",
            " |  doesnt_match(self, words)\n",
            " |      Which key from the given list doesn't go with the others?\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      words : list of str\n",
            " |          List of keys.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      str\n",
            " |          The key further away from the mean of all keys.\n",
            " |  \n",
            " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, similarity_function='most_similar')\n",
            " |      Compute performance of the model on an analogy test set.\n",
            " |      \n",
            " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
            " |      plus there's one aggregate summary at the end.\n",
            " |      \n",
            " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
            " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      analogies : str\n",
            " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
            " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
            " |      restrict_vocab : int, optional\n",
            " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
            " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
            " |          in modern word embedding models).\n",
            " |      case_insensitive : bool, optional\n",
            " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
            " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
            " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
            " |          (also the most frequent if vocabulary is sorted) is taken.\n",
            " |      dummy4unknown : bool, optional\n",
            " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
            " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
            " |      similarity_function : str, optional\n",
            " |          Function name used for similarity calculation.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          The overall evaluation score on the entire evaluation set\n",
            " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
            " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
            " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
            " |          keys 'correct' and 'incorrect'.\n",
            " |  \n",
            " |  evaluate_word_pairs(self, pairs, delimiter='\\t', encoding='utf8', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
            " |      Compute correlation of the model with human similarity judgments.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      More datasets can be found at\n",
            " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
            " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      pairs : str\n",
            " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
            " |          See `test/test_data/wordsim353.tsv` as example.\n",
            " |      delimiter : str, optional\n",
            " |          Separator in `pairs` file.\n",
            " |      restrict_vocab : int, optional\n",
            " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
            " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
            " |          in modern word embedding models).\n",
            " |      case_insensitive : bool, optional\n",
            " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
            " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
            " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
            " |          (also the most frequent if vocabulary is sorted) is taken.\n",
            " |      dummy4unknown : bool, optional\n",
            " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
            " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      pearson : tuple of (float, float)\n",
            " |          Pearson correlation coefficient with 2-tailed p-value.\n",
            " |      spearman : tuple of (float, float)\n",
            " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
            " |          similarities produced by the model itself, with 2-tailed p-value.\n",
            " |      oov_ratio : float\n",
            " |          The ratio of pairs with unknown words.\n",
            " |  \n",
            " |  fill_norms(self, force=False)\n",
            " |      Ensure per-vector norms are available.\n",
            " |      \n",
            " |      Any code which modifies vectors should ensure the accompanying norms are\n",
            " |      either recalculated or 'None', to trigger a full recalculation later on-request.\n",
            " |  \n",
            " |  get_index(self, key, default=None)\n",
            " |      Return the integer index (slot/position) where the given key's vector is stored in the\n",
            " |      backing vectors array.\n",
            " |  \n",
            " |  get_mean_vector(self, keys, weights=None, pre_normalize=True, post_normalize=False, ignore_missing=True)\n",
            " |      Get the mean vector for a given list of keys.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      keys : list of (str or int or ndarray)\n",
            " |          Keys specified by string or int ids or numpy array.\n",
            " |      weights : list of float or numpy.ndarray, optional\n",
            " |          1D array of same size of `keys` specifying the weight for each key.\n",
            " |      pre_normalize : bool, optional\n",
            " |          Flag indicating whether to normalize each keyvector before taking mean.\n",
            " |          If False, individual keyvector will not be normalized.\n",
            " |      post_normalize: bool, optional\n",
            " |          Flag indicating whether to normalize the final mean vector.\n",
            " |          If True, normalized mean vector will be return.\n",
            " |      ignore_missing : bool, optional\n",
            " |          If False, will raise error if a key doesn't exist in vocabulary.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      \n",
            " |      numpy.ndarray\n",
            " |          Mean vector for the list of keys.\n",
            " |      \n",
            " |      Raises\n",
            " |      ------\n",
            " |      \n",
            " |      ValueError\n",
            " |          If the size of the list of `keys` and `weights` doesn't match.\n",
            " |      KeyError\n",
            " |          If any of the key doesn't exist in vocabulary and `ignore_missing` is false.\n",
            " |  \n",
            " |  get_normed_vectors(self)\n",
            " |      Get all embedding vectors normalized to unit L2 length (euclidean), as a 2D numpy array.\n",
            " |      \n",
            " |      To see which key corresponds to which vector = which array row, refer\n",
            " |      to the :attr:`~gensim.models.keyedvectors.KeyedVectors.index_to_key` attribute.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      numpy.ndarray:\n",
            " |          2D numpy array of shape ``(number_of_keys, embedding dimensionality)``, L2-normalized\n",
            " |          along the rows (key vectors).\n",
            " |  \n",
            " |  get_vecattr(self, key, attr)\n",
            " |      Get attribute value associated with given key.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      key : str\n",
            " |          Vector key for which to fetch the attribute value.\n",
            " |      attr : str\n",
            " |          Name of the additional attribute to fetch for the given key.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      \n",
            " |      object\n",
            " |          Value of the additional attribute fetched for the given key.\n",
            " |  \n",
            " |  get_vector(self, key, norm=False)\n",
            " |      Get the key's vector, as a 1D numpy array.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      key : str\n",
            " |          Key for vector to return.\n",
            " |      norm : bool, optional\n",
            " |          If True, the resulting vector will be L2-normalized (unit Euclidean length).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      \n",
            " |      numpy.ndarray\n",
            " |          Vector for the specified key.\n",
            " |      \n",
            " |      Raises\n",
            " |      ------\n",
            " |      \n",
            " |      KeyError\n",
            " |          If the given key doesn't exist.\n",
            " |  \n",
            " |  has_index_for(self, key)\n",
            " |      Can this model return a single index for this key?\n",
            " |      \n",
            " |      Subclasses that synthesize vectors for out-of-vocabulary words (like\n",
            " |      :class:`~gensim.models.fasttext.FastText`) may respond True for a\n",
            " |      simple `word in wv` (`__contains__()`) check but False for this\n",
            " |      more-specific check.\n",
            " |  \n",
            " |  init_sims(self, replace=False)\n",
            " |      Precompute data helpful for bulk similarity calculations.\n",
            " |      \n",
            " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms` now preferred for this purpose.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      replace : bool, optional\n",
            " |          If True - forget the original vectors and only keep the normalized ones.\n",
            " |      \n",
            " |      Warnings\n",
            " |      --------\n",
            " |      \n",
            " |      You **cannot sensibly continue training** after doing a replace on a model's\n",
            " |      internal KeyedVectors, and a replace is no longer necessary to save RAM. Do not use this method.\n",
            " |  \n",
            " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
            " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
            " |      where it intersects with the current vocabulary.\n",
            " |      \n",
            " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
            " |      non-intersecting words are left alone.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          The file path to load the vectors from.\n",
            " |      lockf : float, optional\n",
            " |          Lock-factor value to be set for any imported word-vectors; the\n",
            " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
            " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
            " |      binary : bool, optional\n",
            " |          If True, `fname` is in the binary word2vec C format.\n",
            " |      encoding : str, optional\n",
            " |          Encoding of `text` for `unicode` function (python2 only).\n",
            " |      unicode_errors : str, optional\n",
            " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
            " |  \n",
            " |  most_similar(self, positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None)\n",
            " |      Find the top-N most similar keys.\n",
            " |      Positive keys contribute positively towards the similarity, negative keys negatively.\n",
            " |      \n",
            " |      This method computes cosine similarity between a simple mean of the projection\n",
            " |      weight vectors of the given keys and the vectors for each key in the model.\n",
            " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
            " |      word2vec implementation.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      positive : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
            " |          List of keys that contribute positively. If tuple, second element specifies the weight (default `1.0`)\n",
            " |      negative : list of (str or int or ndarray) or list of ((str,float) or (int,float) or (ndarray,float)), optional\n",
            " |          List of keys that contribute negatively. If tuple, second element specifies the weight (default `-1.0`)\n",
            " |      topn : int or None, optional\n",
            " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
            " |          then similarities for all keys are returned.\n",
            " |      clip_start : int\n",
            " |          Start clipping index.\n",
            " |      clip_end : int\n",
            " |          End clipping index.\n",
            " |      restrict_vocab : int, optional\n",
            " |          Optional integer which limits the range of vectors which\n",
            " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
            " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
            " |          meaningful if you've sorted the vocabulary by descending frequency.) If\n",
            " |          specified, overrides any values of ``clip_start`` or ``clip_end``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (str, float) or numpy.array\n",
            " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
            " |          When `topn` is None, then similarities for all keys are returned as a\n",
            " |          one-dimensional numpy array with the size of the vocabulary.\n",
            " |  \n",
            " |  most_similar_cosmul(self, positive=None, negative=None, topn=10, restrict_vocab=None)\n",
            " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
            " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
            " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
            " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
            " |      In the common analogy-solving case, of two positive and one negative examples,\n",
            " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
            " |      \n",
            " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
            " |      respectively - a potentially sensible but untested extension of the method.\n",
            " |      With a single positive example, rankings will be the same as in the default\n",
            " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.most_similar`.\n",
            " |      \n",
            " |      Allows calls like most_similar_cosmul('dog', 'cat'), as a shorthand for\n",
            " |      most_similar_cosmul(['dog'], ['cat']) where 'dog' is positive and 'cat' negative\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      positive : list of str, optional\n",
            " |          List of words that contribute positively.\n",
            " |      negative : list of str, optional\n",
            " |          List of words that contribute negatively.\n",
            " |      topn : int or None, optional\n",
            " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
            " |          then similarities for all words are returned.\n",
            " |      restrict_vocab : int or None, optional\n",
            " |          Optional integer which limits the range of vectors which are searched for most-similar values.\n",
            " |          For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
            " |          This may be meaningful if vocabulary is sorted by descending frequency.\n",
            " |      \n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (str, float) or numpy.array\n",
            " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
            " |          When `topn` is None, then similarities for all words are returned as a\n",
            " |          one-dimensional numpy array with the size of the vocabulary.\n",
            " |  \n",
            " |  most_similar_to_given(self, key1, keys_list)\n",
            " |      Get the `key` from `keys_list` most similar to `key1`.\n",
            " |  \n",
            " |  n_similarity(self, ws1, ws2)\n",
            " |      Compute cosine similarity between two sets of keys.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      ws1 : list of str\n",
            " |          Sequence of keys.\n",
            " |      ws2: list of str\n",
            " |          Sequence of keys.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      numpy.ndarray\n",
            " |          Similarities between `ws1` and `ws2`.\n",
            " |  \n",
            " |  rank(self, key1, key2)\n",
            " |      Rank of the distance of `key2` from `key1`, in relation to distances of all keys from `key1`.\n",
            " |  \n",
            " |  rank_by_centrality(self, words, use_norm=True)\n",
            " |      Rank the given words by similarity to the centroid of all the words.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      words : list of str\n",
            " |          List of keys.\n",
            " |      use_norm : bool, optional\n",
            " |          Whether to calculate centroid using unit-normed vectors; default True.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (float, str)\n",
            " |          Ranked list of (similarity, key), most-similar to the centroid first.\n",
            " |  \n",
            " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
            " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
            " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
            " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
            " |      \n",
            " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
            " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
            " |      any arbitrary word pairs.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      wa: str\n",
            " |          Word for which we have to look top-n similar word.\n",
            " |      wb: str\n",
            " |          Word for which we evaluating relative cosine similarity with wa.\n",
            " |      topn: int, optional\n",
            " |          Number of top-n similar words to look with respect to wa.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      numpy.float64\n",
            " |          Relative cosine similarity between wa and wb.\n",
            " |  \n",
            " |  resize_vectors(self, seed=0)\n",
            " |      Make underlying vectors match index_to_key size; random-initialize any new rows.\n",
            " |  \n",
            " |  save(self, *args, **kwargs)\n",
            " |      Save KeyedVectors to a file.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname_or_handle : str\n",
            " |          Path to the output file.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.load`\n",
            " |          Load a previously saved model.\n",
            " |  \n",
            " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None, write_header=True, prefix='', append=False, sort_attr='count')\n",
            " |      Store the input-hidden weight matrix in the same format used by the original\n",
            " |      C word2vec-tool, for compatibility.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          File path to save the vectors to.\n",
            " |      fvocab : str, optional\n",
            " |          File path to save additional vocabulary information to. `None` to not store the vocabulary.\n",
            " |      binary : bool, optional\n",
            " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
            " |      total_vec : int, optional\n",
            " |          Explicitly specify total number of vectors\n",
            " |          (in case word vectors are appended with document vectors afterwards).\n",
            " |      write_header : bool, optional\n",
            " |          If False, don't write the 1st line declaring the count of vectors and dimensions.\n",
            " |          This is the format used by e.g. gloVe vectors.\n",
            " |      prefix : str, optional\n",
            " |          String to prepend in front of each stored word. Default = no prefix.\n",
            " |      append : bool, optional\n",
            " |          If set, open `fname` in `ab` mode instead of the default `wb` mode.\n",
            " |      sort_attr : str, optional\n",
            " |          Sort the output vectors in descending order of this attribute. Default: most frequent keys first.\n",
            " |  \n",
            " |  set_vecattr(self, key, attr, val)\n",
            " |      Set attribute associated with the given key to value.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      \n",
            " |      key : str\n",
            " |          Store the attribute for this vector key.\n",
            " |      attr : str\n",
            " |          Name of the additional attribute to store for the given key.\n",
            " |      val : object\n",
            " |          Value of the additional attribute to store for the given key.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      \n",
            " |      None\n",
            " |  \n",
            " |  similar_by_key(self, key, topn=10, restrict_vocab=None)\n",
            " |      Find the top-N most similar keys.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key : str\n",
            " |          Key\n",
            " |      topn : int or None, optional\n",
            " |          Number of top-N similar keys to return. If topn is None, similar_by_key returns\n",
            " |          the vector of similarity scores.\n",
            " |      restrict_vocab : int, optional\n",
            " |          Optional integer which limits the range of vectors which\n",
            " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
            " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
            " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (str, float) or numpy.array\n",
            " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
            " |          When `topn` is None, then similarities for all keys are returned as a\n",
            " |          one-dimensional numpy array with the size of the vocabulary.\n",
            " |  \n",
            " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
            " |      Find the top-N most similar keys by vector.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      vector : numpy.array\n",
            " |          Vector from which similarities are to be computed.\n",
            " |      topn : int or None, optional\n",
            " |          Number of top-N similar keys to return, when `topn` is int. When `topn` is None,\n",
            " |          then similarities for all keys are returned.\n",
            " |      restrict_vocab : int, optional\n",
            " |          Optional integer which limits the range of vectors which\n",
            " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
            " |          only check the first 10000 key vectors in the vocabulary order. (This may be\n",
            " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (str, float) or numpy.array\n",
            " |          When `topn` is int, a sequence of (key, similarity) is returned.\n",
            " |          When `topn` is None, then similarities for all keys are returned as a\n",
            " |          one-dimensional numpy array with the size of the vocabulary.\n",
            " |  \n",
            " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
            " |      Compatibility alias for similar_by_key().\n",
            " |  \n",
            " |  similarity(self, w1, w2)\n",
            " |      Compute cosine similarity between two keys.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      w1 : str\n",
            " |          Input key.\n",
            " |      w2 : str\n",
            " |          Input key.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Cosine similarity between `w1` and `w2`.\n",
            " |  \n",
            " |  similarity_unseen_docs(self, *args, **kwargs)\n",
            " |  \n",
            " |  sort_by_descending_frequency(self)\n",
            " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
            " |  \n",
            " |  unit_normalize_all(self)\n",
            " |      Destructively scale all vectors to unit-length.\n",
            " |      \n",
            " |      You cannot sensibly continue training after such a step.\n",
            " |  \n",
            " |  vectors_for_all(self, keys: Iterable, allow_inference: bool = True, copy_vecattrs: bool = False) -> 'KeyedVectors'\n",
            " |      Produce vectors for all given keys as a new :class:`KeyedVectors` object.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The keys will always be deduplicated. For optimal performance, you should not pass entire\n",
            " |      corpora to the method. Instead, you should construct a dictionary of unique words in your\n",
            " |      corpus:\n",
            " |      \n",
            " |      >>> from collections import Counter\n",
            " |      >>> import itertools\n",
            " |      >>>\n",
            " |      >>> from gensim.models import FastText\n",
            " |      >>> from gensim.test.utils import datapath, common_texts\n",
            " |      >>>\n",
            " |      >>> model_corpus_file = datapath('lee_background.cor')  # train word vectors on some corpus\n",
            " |      >>> model = FastText(corpus_file=model_corpus_file, vector_size=20, min_count=1)\n",
            " |      >>> corpus = common_texts  # infer word vectors for words from another corpus\n",
            " |      >>> word_counts = Counter(itertools.chain.from_iterable(corpus))  # count words in your corpus\n",
            " |      >>> words_by_freq = (k for k, v in word_counts.most_common())\n",
            " |      >>> word_vectors = model.wv.vectors_for_all(words_by_freq)  # create word-vectors for words in your corpus\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      keys : iterable\n",
            " |          The keys that will be vectorized.\n",
            " |      allow_inference : bool, optional\n",
            " |          In subclasses such as :class:`~gensim.models.fasttext.FastTextKeyedVectors`,\n",
            " |          vectors for out-of-vocabulary keys (words) may be inferred. Default is True.\n",
            " |      copy_vecattrs : bool, optional\n",
            " |          Additional attributes set via the :meth:`KeyedVectors.set_vecattr` method\n",
            " |          will be preserved in the produced :class:`KeyedVectors` object. Default is False.\n",
            " |          To ensure that *all* the produced vectors will have vector attributes assigned,\n",
            " |          you should set `allow_inference=False`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      keyedvectors : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
            " |          Vectors for all the given keys.\n",
            " |  \n",
            " |  wmdistance(self, document1, document2, norm=True)\n",
            " |      Compute the Word Mover's Distance between two documents.\n",
            " |      \n",
            " |      When using this code, please consider citing the following papers:\n",
            " |      \n",
            " |      * `Rémi Flamary et al. \"POT: Python Optimal Transport\"\n",
            " |        <https://jmlr.org/papers/v22/20-451.html>`_\n",
            " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
            " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      document1 : list of str\n",
            " |          Input document.\n",
            " |      document2 : list of str\n",
            " |          Input document.\n",
            " |      norm : boolean\n",
            " |          Normalize all word vectors to unit length before computing the distance?\n",
            " |          Defaults to True.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Word Mover's distance between `document1` and `document2`.\n",
            " |      \n",
            " |      Warnings\n",
            " |      --------\n",
            " |      This method only works if `POT <https://pypi.org/project/POT/>`_ is installed.\n",
            " |      \n",
            " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
            " |      will be returned.\n",
            " |      \n",
            " |      Raises\n",
            " |      ------\n",
            " |      ImportError\n",
            " |          If `POT <https://pypi.org/project/POT/>`_  isn't installed.\n",
            " |  \n",
            " |  word_vec(self, *args, **kwargs)\n",
            " |      Compatibility alias for get_vector(); must exist so subclass calls reach subclass get_vector().\n",
            " |  \n",
            " |  words_closer_than(self, word1, word2)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>, no_header=False) from builtins.type\n",
            " |      Load KeyedVectors from a file produced by the original C word2vec-tool format.\n",
            " |      \n",
            " |      Warnings\n",
            " |      --------\n",
            " |      The information stored in the file is incomplete (the binary tree is missing),\n",
            " |      so while you can query for word similarity etc., you cannot continue training\n",
            " |      with a model loaded this way.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          The file path to the saved word2vec-format file.\n",
            " |      fvocab : str, optional\n",
            " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
            " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
            " |      binary : bool, optional\n",
            " |          If True, indicates whether the data is in binary word2vec format.\n",
            " |      encoding : str, optional\n",
            " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
            " |      unicode_errors : str, optional\n",
            " |          default 'strict', is a string suitable to be passed as the `errors`\n",
            " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
            " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
            " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
            " |      limit : int, optional\n",
            " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
            " |          None, means read all.\n",
            " |      datatype : type, optional\n",
            " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
            " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
            " |      no_header : bool, optional\n",
            " |          Default False means a usual word2vec-format file, with a 1st line declaring the count of\n",
            " |          following vectors & number of dimensions. If True, the file is assumed to lack a declaratory\n",
            " |          (vocab_size, vector_size) header and instead start with the 1st vector, and an extra\n",
            " |          reading-pass will be used to discover the number of vectors. Works only with `binary=False`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
            " |          Loaded model.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  cosine_similarities(vector_1, vectors_all)\n",
            " |      Compute cosine similarities between one vector and a set of other vectors.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      vector_1 : numpy.ndarray\n",
            " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
            " |      vectors_all : numpy.ndarray\n",
            " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      numpy.ndarray\n",
            " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
            " |  \n",
            " |  log_accuracy(section)\n",
            " |  \n",
            " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  index2entity\n",
            " |  \n",
            " |  index2word\n",
            " |  \n",
            " |  vectors_norm\n",
            " |  \n",
            " |  vocab\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from gensim.utils.SaveLoad:\n",
            " |  \n",
            " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
            " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
            " |      optionally log the event at `log_level`.\n",
            " |      \n",
            " |      Events are important moments during the object's life, such as \"model created\",\n",
            " |      \"model saved\", \"model loaded\", etc.\n",
            " |      \n",
            " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
            " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
            " |      but is useful during debugging and support.\n",
            " |      \n",
            " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
            " |      will not record events into `self.lifecycle_events` then.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      event_name : str\n",
            " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
            " |      event : dict\n",
            " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
            " |          Can be empty.\n",
            " |      \n",
            " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
            " |      \n",
            " |          - `datetime`: the current date & time\n",
            " |          - `gensim`: the current Gensim version\n",
            " |          - `python`: the current Python version\n",
            " |          - `platform`: the current platform\n",
            " |          - `event`: the name of this event\n",
            " |      log_level : int\n",
            " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from gensim.utils.SaveLoad:\n",
            " |  \n",
            " |  load(fname, mmap=None) from builtins.type\n",
            " |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          Path to file that contains needed object.\n",
            " |      mmap : str, optional\n",
            " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
            " |          via mmap (shared memory) using `mmap='r'.\n",
            " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`~gensim.utils.SaveLoad.save`\n",
            " |          Save object to file.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      object\n",
            " |          Object loaded from `fname`.\n",
            " |      \n",
            " |      Raises\n",
            " |      ------\n",
            " |      AttributeError\n",
            " |          When called on an object instance instead of class (this is a class method).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAWnHY5FE7j2",
        "outputId": "e65fa8dc-577b-4fdd-f4af-c719f7324807"
      },
      "source": [
        "# Deal with an out of dictionary word: Михаил (Michail)\n",
        "if 'Михаил' in model:\n",
        "    print(model['Михаил'].shape)\n",
        "else:\n",
        "    print('{0} is an out of dictionary word'.format('Михаил'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Михаил is an out of dictionary word\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwj15LOLE3fX",
        "outputId": "8ac49ec3-8102-4d76-d7a8-a48e16a499b3"
      },
      "source": [
        "# vector similarity\n",
        "model.most_similar(\"jaguar\", topn=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('xk', 0.7880734205245972),\n",
              " ('rover', 0.775028645992279),\n",
              " ('falcon', 0.7731289267539978),\n",
              " ('xjs', 0.7703999876976013),\n",
              " ('xkr', 0.7470771670341492),\n",
              " ('xj6', 0.7399177551269531),\n",
              " ('puma', 0.7315992116928101),\n",
              " ('xk8', 0.7143670320510864),\n",
              " ('xj', 0.7063968181610107),\n",
              " ('mustang', 0.7020478844642639),\n",
              " ('sepecat', 0.6995052695274353),\n",
              " ('sedan', 0.6786041259765625),\n",
              " ('xk120', 0.6775456070899963),\n",
              " ('xj220', 0.6709454655647278),\n",
              " ('e-type', 0.668567419052124),\n",
              " ('cadillac', 0.6671769618988037),\n",
              " ('sega', 0.6640884280204773),\n",
              " ('xjr', 0.6508543491363525),\n",
              " ('mondeo', 0.6488943696022034),\n",
              " ('v12', 0.647078812122345)]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iW0JfJAFBbN",
        "outputId": "37339fc7-7b16-438e-d6fd-9b2f0262f48c"
      },
      "source": [
        "# Some predefined functions that show content related information for given words\n",
        "print(model.most_similar(positive=['woman', 'king'], negative=['man']))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904), ('princess', 0.7424570322036743), ('kingdom', 0.7337412238121033), ('monarch', 0.721449077129364), ('eldest', 0.7184861898422241), ('widow', 0.7099431157112122)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some predefined functions that show content related information for given words\n",
        "print(model.most_similar(positive=['britain', 'berlin'], negative=['germany']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asuw8Odo62zt",
        "outputId": "8f540a5b-2c55-4bb4-dc48-8ad25177f26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('london', 0.8685765266418457), ('british', 0.7753665447235107), ('sydney', 0.6951888203620911), ('opened', 0.6935645341873169), ('edinburgh', 0.690578281879425), ('blair', 0.6770884394645691), ('prohertrib', 0.6705284118652344), ('paris', 0.6646957993507385), ('international', 0.6595639586448669), ('vienna', 0.6526981592178345)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some predefined functions that show content related information for given words\n",
        "print(model.most_similar(positive=['france', 'beer'], negative=['germany']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1BlIY7Zp4eM",
        "outputId": "271dda3c-6853-4c87-c372-86f9e5de73c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('champagne', 0.851976752281189), ('drinks', 0.834439754486084), ('drink', 0.8175836205482483), ('beers', 0.8047526478767395), ('bottled', 0.7993912696838379), ('wine', 0.7751240134239197), ('vodka', 0.7396015524864197), ('beverages', 0.739106297492981), ('brewed', 0.7388973236083984), ('ale', 0.7360824346542358)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some predefined functions that show content related information for given words\n",
        "print(model.most_similar(positive=['jaguar'], negative=['car']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQlVTCkaBwJ8",
        "outputId": "1db231c1-27e7-4386-e7b2-46cdbac5478a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('yellow-eyed', 0.7645885944366455), ('xj8', 0.7144027352333069), ('xjs', 0.7072294354438782), ('xjr', 0.7008213400840759), ('reho', 0.7004984021186829), ('xj6', 0.6978343725204468), ('sepecat', 0.6680499911308289), ('gfc', 0.6639629006385803), ('xj220', 0.6625925898551941), ('ouimet', 0.6615090370178223)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = (model['city']+model['britain']+model['westminster'])\n",
        "print(cosine_similarity([x], [model['london']]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvVKaO8994wN",
        "outputId": "22fe6902-6721-459f-857e-12de9ce11975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.8388381]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " model.similar_by_vector(x, topn=10, restrict_vocab=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbn1r8Z8-9vg",
        "outputId": "a72dfb4f-2253-420e-9347-d5ef94f9cb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('london', 0.8388381004333496),\n",
              " ('in', 0.8380187153816223),\n",
              " ('opened', 0.8023003935813904),\n",
              " ('west', 0.8015338778495789),\n",
              " ('city', 0.7946631908416748),\n",
              " ('part', 0.7911143898963928),\n",
              " ('where', 0.7886185050010681),\n",
              " ('east', 0.7878355979919434),\n",
              " ('held', 0.7812442779541016),\n",
              " ('new', 0.7798982858657837)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.doesnt_match(\"death delight delirium desire despair destiny destruction dream\".split()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wBaE6P97SLr",
        "outputId": "341e2c2b-50c3-44b8-8b1f-0b6d3ef99c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delirium\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_vu2-mlFJb7",
        "outputId": "213385fc-5f91-4781-ced4-616140ee0d31"
      },
      "source": [
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cereal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbF_BI-WFLt9",
        "outputId": "e5538cd3-5a07-4845-cef7-35024d3d1cf1"
      },
      "source": [
        "print(model.similarity('woman', 'man'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8860338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YesZGg4rGhY3"
      },
      "source": [
        "# Je v tom magie?\n",
        "\n",
        "https://www.researchgate.net/publication/332679657_Metaconcepts_Isolating_Context_in_Word_Embeddings\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaZZvqEwFOEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef4c4f7-366d-4e2e-a638-321314064621"
      },
      "source": [
        "v1 = model['queen']\n",
        "v2 = model['king']\n",
        "gender_change = v2-v1\n",
        "v3 = model['woman']\n",
        "v4 = model['man']\n",
        "\n",
        "print(\"cosine similarity queen-king\", cosine_similarity([v1], [v2]))\n",
        "print(\"cosine similarity woman-man\", cosine_similarity([v3], [v4]))\n",
        "print(\"cosine similarity gender-change-woman-man\", cosine_similarity([v3+gender_change], [v4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine similarity queen-king [[0.7839043]]\n",
            "cosine similarity woman-man [[0.8860338]]\n",
            "cosine similarity gender-change-woman-man [[0.87060666]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install compress-fasttext[full]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnEz8Bml7qX5",
        "outputId": "c4888632-287a-4ae4-f8ae-24b4ce05103c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: compress-fasttext[full] in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from compress-fasttext[full]) (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from compress-fasttext[full]) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from compress-fasttext[full]) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from compress-fasttext[full]) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->compress-fasttext[full]) (7.0.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->compress-fasttext[full]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->compress-fasttext[full]) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim>=4.0.0->compress-fasttext[full]) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compressed Models\n",
        "Let's try some from https://zenodo.org/records/4905385"
      ],
      "metadata": {
        "id": "DsRgNVyF98UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import compress_fasttext"
      ],
      "metadata": {
        "id": "rBWu0H_e7ttw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = compress_fasttext.models.CompressedFastTextKeyedVectors.load('fasttext-cs-mini')"
      ],
      "metadata": {
        "id": "U2liCmgl7xhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "jZF2Wj8CA7b2",
        "outputId": "f41e2ee1-cd5f-45a9-c2b2-c9d112e2ac8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "compress_fasttext.compress.CompressedFastTextKeyedVectors"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>compress_fasttext.compress.CompressedFastTextKeyedVectors</b><br/>def __init__(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/compress_fasttext/compress.py</a>This class extends FastTextKeyedVectors by fixing several issues:\n",
              "- index2word of a freshly created model is initialized from its vocab\n",
              "- the model does not keep heavy and useless vectors_ngrams_norm\n",
              "- word_vec() method with use_norm applies normalization in the right place</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 18);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.most_similar(\"jaguar\", topn=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG9WCSsmMV6M",
        "outputId": "9a2628da-e033-4fa6-f481-957625742733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('BMW', 0.34397847667296316),\n",
              " ('Romeo', 0.32179035423934116),\n",
              " ('Audi', 0.3180917180385469),\n",
              " ('automobilka', 0.3168419658764127),\n",
              " ('vůz', 0.30489487257924985),\n",
              " ('Johnny', 0.3043852203171347),\n",
              " ('auto', 0.3008478573190045),\n",
              " ('SUV', 0.2990117623485818),\n",
              " ('benzín', 0.29855281755316265),\n",
              " ('plot', 0.2975386581956121),\n",
              " ('Volkswagen', 0.29460507277853926),\n",
              " ('vozů', 0.2932022021556993),\n",
              " ('vozu', 0.29031136779444494),\n",
              " ('Porsche', 0.28977824904327326),\n",
              " ('motor', 0.2885117154496917),\n",
              " ('šampionát', 0.28703590238685495),\n",
              " ('modelů', 0.2863343829436737),\n",
              " ('Ford', 0.28601655518184527),\n",
              " ('Ferrari', 0.28154542819812123),\n",
              " ('Fiat', 0.28141098183224733)]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.most_similar(positive=['Bratislava', 'Vltava'], negative=['Praha'], topn=20)"
      ],
      "metadata": {
        "id": "iXlyyeCJBFgt",
        "outputId": "6f77f93a-1250-4c0b-fb2f-2190a6c0df2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Nitra', 0.37974412759069726),\n",
              " ('Labe', 0.37252080644672436),\n",
              " ('Bratislavě', 0.3632184011816346),\n",
              " ('Morava', 0.339258624023774),\n",
              " ('Slovensku', 0.3282379269035694),\n",
              " ('Slovenska', 0.32423271957688),\n",
              " ('Řeka', 0.31847280587367577),\n",
              " ('Poprad', 0.3160367662461779),\n",
              " ('Bystrica', 0.3135269412236826),\n",
              " ('Slovensko', 0.3053635680676401),\n",
              " ('řeka', 0.3026565769766381),\n",
              " ('Marina', 0.29952029111169826),\n",
              " ('Rádio', 0.2988152612162455),\n",
              " ('Juraj', 0.2979232860685962),\n",
              " ('Smetana', 0.2961783598070842),\n",
              " ('Moravy', 0.2953246306538546),\n",
              " ('River', 0.29505461895485174),\n",
              " ('Slovan', 0.29486439585920615),\n",
              " ('Košice', 0.29392294467764807),\n",
              " ('Trnava', 0.29141266951356204)]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(m.doesnt_match([\"běžet\", \"ležet\", \"chodit\", \"tančit\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oUnLUBHMCpr",
        "outputId": "0ccb5493-da5f-4115-8114-3db33b51dd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ležet\n"
          ]
        }
      ]
    }
  ]
}